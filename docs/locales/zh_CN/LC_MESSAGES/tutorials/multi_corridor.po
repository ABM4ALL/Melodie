# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, open excel_source
# This file is distributed under the same license as the Melodie package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Melodie \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-12-17 11:18+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/tutorials/multi_corridor.rst:6 58b2986c204c4036970f1397399715e7
msgid "MultiCorridor"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:8 a1831cff2cd24d05b87b3d6e0ebf4431
msgid ""
"MultiCorridor extends RLlib's `simple corridor <https://github.com/ray-"
"project/ray/blob/master/rllib/examples/custom_env.py#L65>`_, wherein "
"agents must learn to move to the right in a one-dimensonal corridor to "
"reach the end. Our implementation provides the ability to instantiate "
"multiple agents in the simulation and restricts agents from occupying the"
" same square. Every agent is homogeneous: they all have the same action "
"space, observation space, and objective function."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:18
#: 3f83431000dd41a79a111699d28a6ace 58d62aaff1434777baf284fcf8387e5d
msgid ""
"Animation of agents moving left and right in a corridor until they reach "
"the end."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:20
#: 5301e350005a445d945f4cd3e3d7be52
msgid ""
"This tutorial uses the `MultiCorridor simulation "
"<https://github.com/LLNL/Abmarl/blob/main/abmarl/sim/corridor/multi_corridor.py>`_"
" and the `MultiCorridor configuration "
"<https://github.com/LLNL/Abmarl/blob/main/examples/multi_corridor_example.py>`_."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:24
#: f2741cc5a6474ed989d9db4345a16e8b
msgid "Creating the MultiCorridor Simulation"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:27
#: 8145248e5077404da5fa3ce1019ce0a0
msgid "The Agents in the Simulation"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:28
#: d5767eaf507d426b8ca68d3d26084506
msgid ""
"It's helpful to start by thinking about what we want the agents to learn "
"and what information they will need in order to learn it. In this "
"tutorial, we want to train agents that can reach the end of a one-"
"dimensional corridor without bumping into each other. Therefore, agents "
"should be able to move left, move right, and stay still. In order to move"
" to the end of the corridor without bumping into each other, they will "
"need to see their own position and if the squares near them are occupied."
" Finally, we need to decide how to reward the agents. There are many ways"
" we can do this, and we should at least capture the following:"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:37
#: b0d74cd3f69740f2be48a77201e60844
msgid "The agent should be rewarded for reaching the end of the corridor."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:38
#: ddbe5da0f71543ac98237487e83005b5
msgid "The agent should be penalized for bumping into other agents."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:39
#: fe53145fe680408fbb9128674d92aad2
msgid "The agent should be penalized for taking too long."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:41
#: 65f486197b16488db5d0ec4b30424b8c
msgid ""
"Since all our agents are homogeneous, we can create them in the Agent "
"Based Simulation itself, like so:"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:77
#: 0f46b5296ba14c8f9a5b3cf496d11933
msgid ""
"Here, notice how the agents' `observation_space` is a `dict` rather than "
"a `gym.space.Dict`. That's okay because our `Agent` class can convert a "
"`dict of gym spaces` into a `Dict` when ``finalize`` is called at the end"
" of ``__init__``."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:83
#: e94dbf5f4acf448fb65d4153e93be1a0
msgid "Resetting the Simulation"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:85
#: 66092e9f9a484067badea9a4ba00277f
msgid ""
"At the beginning of each episode, we want the agents to be randomly "
"positioned throughout the corridor without occupying the same squares. We"
" must give each agent a position attribute at reset. We will also create "
"a data structure that captures which agent is in which cell so that we "
"don't have to do a search for nearby agents but can directly index the "
"space. Finally, we must track the agents' rewards."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:107
#: 558db3c041a44a059addc4932fd36254
msgid "Stepping the Simulation"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:109
#: 0c0911e4edf1482cb28783a880439933
msgid ""
"The simulation is driven by the agents' actions because there are no "
"other dynamics. Thus, the MultiCorridor Simulation only concerns itself "
"with processing the agents' actions at each step. For each agent, we'll "
"capture the following cases:"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:113
#: 44eee2d928204e76bebb6ac2c737b34c
msgid "An agent attempts to move to a space that is unoccupied."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:114
#: 0f8376bd49c945a2862e004668fd1726
msgid "An agent attempts to move to a space that is already occupied."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:115
#: e51d5988f712450d8c567c7287e0fedd
msgid ""
"An agent attempts to move to the right-most space (the end) of the "
"corridor."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:158
#: 84091bca76374e08b235bfd751f68ac8
msgid ""
"Our reward schema reveals a training dynamic that is not present in "
"single-agent simulations: an agent's reward does not entirely depend on "
"its own interaction with the simulation but can be affected by other "
"agents' actions. In this case, agents are slightly penalized for being "
"\"bumped into\" when other agents attempt to move onto their square, even"
" though the \"offended\" agent did not directly cause the collision. This"
" is discussed in MARL literature and captured in the way we have designed"
" our Simulation Managers. In Abmarl, we favor capturing the rewards as "
"part of the simulation's state and only \"flushing\" them once they "
"rewards are asked for in ``get_reward``."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:170
#: 408f7f8ebb894a7aaae4817e1b3b83fe
msgid ""
"We have not needed to consider the order in which the simulation "
"processes actions. Our simulation simply provides the capabilities to "
"process *any* agent's action, and we can use `Simulation Managers` to "
"impose an order. This shows the flexibility of our design. In this "
"tutorial, we will use the `TurnBasedManager`, but we can use any "
"`SimulationManager`."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:177
#: 602fb1e84a804847b1bee1853bfc6a29
msgid "Querying Simulation State"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:179
#: 68f0c28d6055432aa2f93c2a1f2ee8c5
msgid ""
"The trainer needs to see how agents' actions impact the simulation's "
"state. They do so via getters, which we define below."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:218
#: 0ec0fb43a0bf42758e668073669e9bdd
msgid "Rendering for Visualization"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:219
#: 5de5e95d080b430c9c41eebf8ea4541f
msgid ""
"Finally, it's often useful to be able to visualize a simulation as it "
"steps through an episode. We can do this via the render funciton."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:247
#: 088a41b84c3a4e40836c45dad21d3c19
msgid "Training the MultiCorridor Simulation"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:249
#: 44f32d4f8def4bf38e69e7dd1a399e13
msgid ""
"Now that we have created the simulation and agents, we can create a "
"configuration file for training."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:253
#: c1b1add879534c5e928b2fb7738eec5d
msgid "Simulation Setup"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:255
#: 8cf4c766ed054869822b5c2794f504cb
msgid ""
"We'll start by setting up the simulation we have just built. Then we'll "
"choose a Simulation Manager. Abmarl comes with two built-In managers: "
"`TurnBasedManager`, where only a single agent takes a turn per step, and "
"`AllStepManager`, where all non-done agents take a turn per step. For "
"this experiment, we'll use the `TurnBasedManager`. Then, we'll wrap the "
"simulation with our `MultiAgentWrapper`, which enables us to connect with"
" RLlib. Finally, we'll register the simulation with RLlib."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:278
#: 75b77625ce63436a9367c2ec7dfaf14e
msgid "Policy Setup"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:280
#: 0ef51e925fc74f49987788446b8d42d1
msgid ""
"Now we want to create the policies and the policy mapping function in our"
" multiagent experiment. Each agent in our simulation is homogeneous: they"
" all have the same observation space, action space, and objective "
"function. Thus, we can create a single policy and map all agents to that "
"policy."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:295
#: 0a81d6f123984ad4acacfdee0413dd2d
msgid "Experiment Parameters"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:297
#: 784ba6f1b694468890a5f8ad4603d023
msgid ""
"Having setup the simulation and policies, we can now bundle all that "
"information into a parameters dictionary that will be read by Abmarl and "
"used to launch RLlib."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:335
#: db83fb9cd5df489c90758e6efad7a1e9
msgid "Command Line interface"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:336
#: 39c0342c75f84a42bf5123b9f2912bd0
msgid ""
"With the configuration file complete, we can utilize the command line "
"interface to train our agents. We simply type ``abmarl train "
"multi_corridor_example.py``, where `multi_corridor_example.py` is the "
"name of our configuration file. This will launch Abmarl, which will "
"process the file and launch RLlib according to the specified parameters. "
"This particular example should take 1-10 minutes to train, depending on "
"your compute capabilities. You can view the performance in real time in "
"tensorboard with ``tensorboard --logdir ~/abmarl_results``."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:346
#: 0a61ada3f8d548be8e7f4b631a717388
msgid "Visualizing the Trained Behaviors"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:347
#: 812b663aac1d4c71bb6cb386d62a6a60
msgid ""
"We can visualize the agents' learned behavior with the ``visualize`` "
"command, which takes as argument the output directory from the training "
"session stored in ``~/abmarl_results``. For example, the command"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:355
#: e60e299713fa4186933053d10056bfa9
msgid ""
"will load the experiment (notice that the directory name is the "
"experiment title from the configuration file appended with a timestamp) "
"and display an animation of 5 episodes. The ``--record`` flag will save "
"the animations as `.mp4` videos in the training directory."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:363
#: 01b97a8511954808b86612a8a502c6a2
msgid "Extra Challenges"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:364
#: 10b1f7ca9fa74911b5b13d59e33c8e11
msgid ""
"Having successfully trained a MARL experiment, we can further explore the"
" agents' behaviors and the training process. Some ideas are:"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:367
#: 8b13547a6db54adda86e04afe8040bfc
msgid ""
"We could enhance the MultiCorridor Simulation so that the \"target\" cell"
" is a different location in each episode."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:369
#: 0648b135d45f4dc4a23acdf23ff4b124
msgid ""
"We could introduce heterogeneous agents with the ability to \"jump over\""
" other agents. With heterogeneous agents, we can nontrivially train "
"multiple policies."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:371
#: 271822ee77f84b2a8a8b3efa51fdd6ca
msgid ""
"We could study how the agents' behaviors differ if they are trained using"
" the `AllStepManager`."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:372
#: 4506cae0629c4bedab66e5b7cdad2586
msgid ""
"We could create our own Simulation Manager so that if an agent causes a "
"collision, it skips its next turn."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:374
#: 4d961dd64ea445bfb6c9444ede04ddd3
msgid ""
"We could do a parameter search over both simulation and algorithm "
"parameters to study how the parameters affect the learned behaviors."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:376
#: 23ec147fd6ea41f08de4708f0169d361
msgid ""
"We could analyze how often agents collide with one another and where "
"those collisions most commonly occur."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:378
#: f377b71092a54d689e29d001c874faac
msgid "And much, much more!"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:380
#: 42217193cd524b969293a991ec1d520c
msgid ""
"As we attempt these extra challenges, we will experience one of Abmarl's "
"strongest features: the ease with which we can modify our experiment file"
" and launch another training job, going through the pipeline from "
"experiment setup to behavior visualization and analysis!"
msgstr ""

