# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, open source
# This file is distributed under the same license as the Melodie package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Melodie \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-05 15:03+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/tutorials/gym.rst:6
msgid "Gym Environment"
msgstr ""

#: ../../source/tutorials/gym.rst:8
msgid ""
"Abmarl can be used with OpenAI Gym environments. In this tutorial, we'll "
"create a training configuration file that trains a gym environment. This "
"tutorial uses the `gym configuration "
"<https://github.com/LLNL/Abmarl/blob/main/examples/gym_example.py>`_."
msgstr ""

#: ../../source/tutorials/gym.rst:14
msgid "Training a Gym Environment"
msgstr ""

#: ../../source/tutorials/gym.rst:17
#: ../../source/tutorials/multi_corridor.rst:253
#: ../../source/tutorials/predator_prey.rst:115
msgid "Simulation Setup"
msgstr ""

#: ../../source/tutorials/gym.rst:19
msgid "We'll start by creating gym's built-in guessing game."
msgstr ""

#: ../../source/tutorials/gym.rst:32
msgid "Even gym's built-in environments need to be registered with RLlib."
msgstr ""

#: ../../source/tutorials/gym.rst:35
#: ../../source/tutorials/multi_corridor.rst:295
#: ../../source/tutorials/predator_prey.rst:133
msgid "Experiment Parameters"
msgstr ""

#: ../../source/tutorials/gym.rst:37
msgid ""
"All training configuration parameters are stored in a dictionary called "
"`params`. Having setup the simualtion, we can now create the `params` "
"dictionary that will be read by Abmarl and used to launch RLlib."
msgstr ""

#: ../../source/tutorials/gym.rst:72
#: ../../source/tutorials/multi_corridor.rst:335
msgid "Command Line interface"
msgstr ""

#: ../../source/tutorials/gym.rst:73
msgid ""
"With the configuration file complete, we can utilize the command line "
"interface to train our agents. We simply type ``abmarl train "
"gym_example.py``, where `gym_example.py` is the name of our configuration"
" file. This will launch Abmarl, which will process the file and launch "
"RLlib according to the specified parameters. This particular example "
"should take 1-10 minutes to train, depending on your compute "
"capabilities. You can view the performance in real time in tensorboard "
"with ``tensorboard --logdir ~/abmarl_results``."
msgstr ""

#: ../../source/tutorials/magpie.rst:6
msgid "Magpie"
msgstr ""

#: ../../source/tutorials/magpie.rst:8
msgid ""
"The prospect of applying MuliAgent Reinforcement Learning algorithms on "
"HPC systems is very attractive. As a first step, we demonstrate that "
"abmarl can be used with `magpie <https://github.com/LLNL/magpie>`_ to "
"create batch jobs for running on multiple compute nodes."
msgstr ""

#: ../../source/tutorials/magpie.rst:15
msgid "Installing Abmarl on HPC systems"
msgstr ""

#: ../../source/tutorials/magpie.rst:17
msgid "Here we'll use conda to install on an HPC system:"
msgstr ""

#: ../../source/tutorials/magpie.rst:19
msgid "Create the conda virtual environment: `conda create --name abmarl`"
msgstr ""

#: ../../source/tutorials/magpie.rst:20
msgid "Activate it: `conda activate abmarl`"
msgstr ""

#: ../../source/tutorials/magpie.rst:21
msgid "Install pip installer: `conda install --name abmarl pip`"
msgstr ""

#: ../../source/tutorials/magpie.rst:22
msgid "Follow :ref:`installation instructions <installation>`"
msgstr ""

#: ../../source/tutorials/magpie.rst:25
msgid "Usage"
msgstr ""

#: ../../source/tutorials/magpie.rst:27
msgid ""
"We demonstrate running the :ref:`PredatorPrey tutorial "
"<tutorial_predator_prey>` using Mapgie."
msgstr ""

#: ../../source/tutorials/magpie.rst:31
msgid "make-runnable"
msgstr ""

#: ../../source/tutorials/magpie.rst:32
msgid ""
"Abmarl's command line interface provides the `make-runnable` subcommand "
"that converts the configuration script into a runnable script and saves "
"it to the same directory."
msgstr ""

#: ../../source/tutorials/magpie.rst:40
msgid "This will create a file called `runnable_predator_prey_training.py`."
msgstr ""

#: ../../source/tutorials/magpie.rst:43
msgid "Magpie flag"
msgstr ""

#: ../../source/tutorials/magpie.rst:44
msgid ""
"The full use of `make-runnable` is seen when it is run with the "
"``--magpie`` flag. This will create a custom magpie script using "
"`magpie's ray default script <https://github.com/LLNL/magpie/blob/master"
"/submission-scripts/script-sbatch-srun/magpie.sbatch-srun-ray>`_ as a "
"starting point. This also adds the correct initialization parameters to "
"`ray.init()` in the `runnable_` script. For example,"
msgstr ""

#: ../../source/tutorials/magpie.rst:55
msgid ""
"will create the `runnable_` script with "
"``ray.init(address=os.environ['MAGPIE_RAY_ADDRESS'])`` and will create a "
"`magpie batch script "
"<https://github.com/LLNL/Abmarl/blob/main/examples/predator_prey/PredatorPrey_magpie"
".sbatch-srun-ray>`_ that is setup to run this example. To launch the "
"batch job, we simply run it from the command line:"
msgstr ""

#: ../../source/tutorials/magpie.rst:65
msgid ""
"The script can be modified to adjust the job parameters, such as the "
"number of compute nodes, the time limit for the job, etc. This can also "
"be done through abmarl via the ``-n`` and ``-t`` options."
msgstr ""

#: ../../source/tutorials/magpie.rst:70
msgid ""
"the `num_workers` parameter in the tune configuration is the number of "
"processors to utilize per compute node, which is the different from the "
"number of compute nodes you are requesting."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:6
msgid "MultiCorridor"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:8
msgid ""
"MultiCorridor extends RLlib's `simple corridor <https://github.com/ray-"
"project/ray/blob/master/rllib/examples/custom_env.py#L65>`_, wherein "
"agents must learn to move to the right in a one-dimensonal corridor to "
"reach the end. Our implementation provides the ability to instantiate "
"multiple agents in the simulation and restricts agents from occupying the"
" same square. Every agent is homogeneous: they all have the same action "
"space, observation space, and objective function."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:18
msgid ""
"Animation of agents moving left and right in a corridor until they reach "
"the end."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:20
msgid ""
"This tutorial uses the `MultiCorridor simulation "
"<https://github.com/LLNL/Abmarl/blob/main/abmarl/sim/corridor/multi_corridor.py>`_"
" and the `MultiCorridor configuration "
"<https://github.com/LLNL/Abmarl/blob/main/examples/multi_corridor_example.py>`_."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:24
msgid "Creating the MultiCorridor Simulation"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:27
#: ../../source/tutorials/predator_prey.rst:30
msgid "The Agents in the Simulation"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:28
msgid ""
"It's helpful to start by thinking about what we want the agents to learn "
"and what information they will need in order to learn it. In this "
"tutorial, we want to train agents that can reach the end of a one-"
"dimensional corridor without bumping into each other. Therefore, agents "
"should be able to move left, move right, and stay still. In order to move"
" to the end of the corridor without bumping into each other, they will "
"need to see their own position and if the squares near them are occupied."
" Finally, we need to decide how to reward the agents. There are many ways"
" we can do this, and we should at least capture the following:"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:37
msgid "The agent should be rewarded for reaching the end of the corridor."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:38
msgid "The agent should be penalized for bumping into other agents."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:39
msgid "The agent should be penalized for taking too long."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:41
msgid ""
"Since all our agents are homogeneous, we can create them in the Agent "
"Based Simulation itself, like so:"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:77
msgid ""
"Here, notice how the agents' `observation_space` is a `dict` rather than "
"a `gym.space.Dict`. That's okay because our `Agent` class can convert a "
"`dict of gym spaces` into a `Dict` when ``finalize`` is called at the end"
" of ``__init__``."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:83
msgid "Resetting the Simulation"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:85
msgid ""
"At the beginning of each episode, we want the agents to be randomly "
"positioned throughout the corridor without occupying the same squares. We"
" must give each agent a position attribute at reset. We will also create "
"a data structure that captures which agent is in which cell so that we "
"don't have to do a search for nearby agents but can directly index the "
"space. Finally, we must track the agents' rewards."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:107
msgid "Stepping the Simulation"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:109
msgid ""
"The simulation is driven by the agents' actions because there are no "
"other dynamics. Thus, the MultiCorridor Simulation only concerns itself "
"with processing the agents' actions at each step. For each agent, we'll "
"capture the following cases:"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:113
msgid "An agent attempts to move to a space that is unoccupied."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:114
msgid "An agent attempts to move to a space that is already occupied."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:115
msgid ""
"An agent attempts to move to the right-most space (the end) of the "
"corridor."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:158
msgid ""
"Our reward schema reveals a training dynamic that is not present in "
"single-agent simulations: an agent's reward does not entirely depend on "
"its own interaction with the simulation but can be affected by other "
"agents' actions. In this case, agents are slightly penalized for being "
"\"bumped into\" when other agents attempt to move onto their square, even"
" though the \"offended\" agent did not directly cause the collision. This"
" is discussed in MARL literature and captured in the way we have designed"
" our Simulation Managers. In Abmarl, we favor capturing the rewards as "
"part of the simulation's state and only \"flushing\" them once they "
"rewards are asked for in ``get_reward``."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:170
msgid ""
"We have not needed to consider the order in which the simulation "
"processes actions. Our simulation simply provides the capabilities to "
"process *any* agent's action, and we can use `Simulation Managers` to "
"impose an order. This shows the flexibility of our design. In this "
"tutorial, we will use the `TurnBasedManager`, but we can use any "
"`SimulationManager`."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:177
msgid "Querying Simulation State"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:179
msgid ""
"The trainer needs to see how agents' actions impact the simulation's "
"state. They do so via getters, which we define below."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:218
msgid "Rendering for Visualization"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:219
msgid ""
"Finally, it's often useful to be able to visualize a simulation as it "
"steps through an episode. We can do this via the render funciton."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:247
msgid "Training the MultiCorridor Simulation"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:249
msgid ""
"Now that we have created the simulation and agents, we can create a "
"configuration file for training."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:255
msgid ""
"We'll start by setting up the simulation we have just built. Then we'll "
"choose a Simulation Manager. Abmarl comes with two built-In managers: "
"`TurnBasedManager`, where only a single agent takes a turn per step, and "
"`AllStepManager`, where all non-done agents take a turn per step. For "
"this experiment, we'll use the `TurnBasedManager`. Then, we'll wrap the "
"simulation with our `MultiAgentWrapper`, which enables us to connect with"
" RLlib. Finally, we'll register the simulation with RLlib."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:278
#: ../../source/tutorials/predator_prey.rst:124
msgid "Policy Setup"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:280
msgid ""
"Now we want to create the policies and the policy mapping function in our"
" multiagent experiment. Each agent in our simulation is homogeneous: they"
" all have the same observation space, action space, and objective "
"function. Thus, we can create a single policy and map all agents to that "
"policy."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:297
msgid ""
"Having setup the simulation and policies, we can now bundle all that "
"information into a parameters dictionary that will be read by Abmarl and "
"used to launch RLlib."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:336
msgid ""
"With the configuration file complete, we can utilize the command line "
"interface to train our agents. We simply type ``abmarl train "
"multi_corridor_example.py``, where `multi_corridor_example.py` is the "
"name of our configuration file. This will launch Abmarl, which will "
"process the file and launch RLlib according to the specified parameters. "
"This particular example should take 1-10 minutes to train, depending on "
"your compute capabilities. You can view the performance in real time in "
"tensorboard with ``tensorboard --logdir ~/abmarl_results``."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:346
msgid "Visualizing the Trained Behaviors"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:347
msgid ""
"We can visualize the agents' learned behavior with the ``visualize`` "
"command, which takes as argument the output directory from the training "
"session stored in ``~/abmarl_results``. For example, the command"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:355
msgid ""
"will load the experiment (notice that the directory name is the "
"experiment title from the configuration file appended with a timestamp) "
"and display an animation of 5 episodes. The ``--record`` flag will save "
"the animations as `.mp4` videos in the training directory."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:363
#: ../../source/tutorials/predator_prey.rst:311
msgid "Extra Challenges"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:364
msgid ""
"Having successfully trained a MARL experiment, we can further explore the"
" agents' behaviors and the training process. Some ideas are:"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:367
msgid ""
"We could enhance the MultiCorridor Simulation so that the \"target\" cell"
" is a different location in each episode."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:369
msgid ""
"We could introduce heterogeneous agents with the ability to \"jump over\""
" other agents. With heterogeneous agents, we can nontrivially train "
"multiple policies."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:371
msgid ""
"We could study how the agents' behaviors differ if they are trained using"
" the `AllStepManager`."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:372
msgid ""
"We could create our own Simulation Manager so that if an agent causes a "
"collision, it skips its next turn."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:374
msgid ""
"We could do a parameter search over both simulation and algorithm "
"parameters to study how the parameters affect the learned behaviors."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:376
msgid ""
"We could analyze how often agents collide with one another and where "
"those collisions most commonly occur."
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:378
msgid "And much, much more!"
msgstr ""

#: ../../source/tutorials/multi_corridor.rst:380
msgid ""
"As we attempt these extra challenges, we will experience one of Abmarl's "
"strongest features: the ease with which we can modify our experiment file"
" and launch another training job, going through the pipeline from "
"experiment setup to behavior visualization and analysis!"
msgstr ""

#: ../../source/tutorials/predator_prey.rst:6
msgid "PredatorPrey"
msgstr ""

#: ../../source/tutorials/predator_prey.rst:8
msgid ""
"PredatorPrey is a multiagent simulation useful for exploring competitve "
"behaviors between groups of agents. Resources \"grow\" in a two-"
"dimensional grid. Prey agents move around the grid harvesting resources, "
"and predator agents move around the grid hunting the prey agents."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:17
msgid "Animation of predator and prey agents in a two-dimensional grid."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:19
msgid ""
"This tutorial uses the `PredatorPrey simulation "
"<https://github.com/LLNL/Abmarl/blob/main/abmarl/sim/predator_prey/predator_prey.py>`_,"
" and the `PredatorPrey configuration "
"<https://github.com/LLNL/Abmarl/blob/main/examples/predator_prey/predator_prey_training.py>`_."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:23
msgid ""
"This tutorial requires seaborn for visualizing the resources. This can be"
" easily added to your virtual environment with ``pip install seaborn``."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:27
msgid "Creating the PredatorPrey Simulation"
msgstr ""

#: ../../source/tutorials/predator_prey.rst:31
msgid ""
"In this tutorial, we will train predators to hunt prey by moving around "
"the grid and attacking them when they are nearby. In order to learn this,"
" they must be able to see a subset of the grid around their position, and"
" they must be able to distinguish between other predators and prey. We "
"will reward the predators as follows:"
msgstr ""

#: ../../source/tutorials/predator_prey.rst:36
msgid "The predator should be rewarded for successfully killing a prey."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:37
msgid ""
"The predator should be penalized for trying to move off the edge of the "
"grid."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:38
msgid "The predator should be penalized for taking too long."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:40
msgid ""
"Concurrently, we will train prey agents to harvest resources while "
"attempting to avoid predators. To learn this, prey must be able to see a "
"subset off the grid around them, both the resources available and any "
"other agents. We will reward the prey as follows:"
msgstr ""

#: ../../source/tutorials/predator_prey.rst:45
msgid "The prey should be rewarded for harvesting resources."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:46
msgid "The prey should be penalized for trying to move off the edge of the grid."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:47
msgid "The prey should be penalized for getting eaten by a predator."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:48
msgid "The prey should be penalized for taking too long."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:50
msgid ""
"In order to accomodate this, we will create two types of Agents, one for "
"Predators and one for Prey. Notice that all agents can move around and "
"view a subset of the grid, so we'll capture this in a parent class and "
"encode the distinction in the agents' respective child classes."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:102
msgid "The PredatorPrey Simulation"
msgstr ""

#: ../../source/tutorials/predator_prey.rst:103
msgid ""
"The PredatorPrey Simulation needs much detailed explanation, which we "
"believe will distract from this tutorial. Suffice it to say that we have "
"created a simulation that works with the above agents and captures our "
"desired features. This simulation can be found in full `in our repo "
"<https://github.com/LLNL/Abmarl/blob/main/abmarl/sim/predator_prey/predator_prey.py>`_."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:109
msgid "Training the Predator Prey Simulation"
msgstr ""

#: ../../source/tutorials/predator_prey.rst:111
msgid ""
"With the PredatorPrey simulation and agents at hand, we can create a "
"configuration file for training."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:116
msgid ""
"Setting up the PredatorPrey simulation requires us to explicity make "
"agents and pass those to the simulation builder. Once we've done that, we"
" can choose which `SimulationManager` to use. In this tutorial, we'll use"
" the `AllStepManager`. Then, we'll wrap the simulation with our "
"`MultiAgentWrapper`, which enables us to connect with RLlib. Finally, "
"we'll register the simulation with RLlib."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:126
msgid ""
"Next, we will create the policies and the policy mapping function. "
"Because predators and prey are competitve, they must train separate "
"polices from one another. Furthermore, since each prey is homogeneous "
"with other prey and each predator with other predators, we can have them "
"train the same policy. Thus, we will have two policies: one for predators"
" and one for prey."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:134
msgid ""
"The last thing is to wrap all the parameters together into a single "
"`params` dictionary. Below is the full configuration file:"
msgstr ""

#: ../../source/tutorials/predator_prey.rst:209
msgid "Using the Command Line"
msgstr ""

#: ../../source/tutorials/predator_prey.rst:212
msgid "Training"
msgstr ""

#: ../../source/tutorials/predator_prey.rst:213
msgid ""
"With the configuration script complete, we can utilize the command line "
"interface to train our predator. We simply type ``abmarl train "
"predator_prey_training.py``, where `predator_prey_training.py` is our "
"configuration file. This will launch Abmarl, which will process the "
"script and launch RLlib according to the specified parameters. This "
"particular example should take about 10 minutes to train, depending on "
"your compute capabilities. You can view the performance in real time in "
"tensorboard with ``tensorboard --logdir ~/abmarl_results``. We can find "
"the rewards associated with the policies on the second page of "
"tensorboard."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:224
msgid "Visualizing"
msgstr ""

#: ../../source/tutorials/predator_prey.rst:225
msgid ""
"Having successfully trained predators to attack prey, we can vizualize "
"the agents' learned behavior with the `visualize` command, which takes as"
" argument the output directory from the training session stored in "
"`~/abmarl_results`. For example, the command"
msgstr ""

#: ../../source/tutorials/predator_prey.rst:235
msgid ""
"will load the training session (notice that the directory name is the "
"experiment title from the configuration script appended with a timestamp)"
" and display an animation of 5 episodes. The `--record` flag will save "
"the animations as `.mp4` videos in the training directory."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:241
msgid "Analyzing"
msgstr ""

#: ../../source/tutorials/predator_prey.rst:242
msgid ""
"We can further investigate the learned behaviors using the `analyze` "
"command along with an analysis script. Analysis scripts implement a `run`"
" command which takes the Simulation and the Trainer as input arguments. "
"We can define any script to further investigate the agents' behavior. In "
"this example, we will craft a script that records how often a predator "
"attacks from each grid square."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:289
msgid "We can run this analysis with"
msgstr ""

#: ../../source/tutorials/predator_prey.rst:295
msgid "which renders the following image for us"
msgstr ""

msgid ""
"Animation of agents moving back and forth in a corridor until they reach "
"the end."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:301
msgid ""
"The heatmap figures indicate that the predators spend most of their time "
"attacking prey from the center of the map and rarely ventures to the "
"corners."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:305
msgid ""
"Creating the analysis script required some in-depth knowledge about the "
"inner workings of the PredatorPrey Simulation. This will likely be needed"
" when analyzing most simulation you work with."
msgstr ""

#: ../../source/tutorials/predator_prey.rst:312
msgid ""
"Having successfully trained the predators to attack prey experiment, we "
"can further explore the agents' behaviors and the training process. For "
"example, you may have noticed that the prey agents didn't seem to learn "
"anything. We may need to improve our reward schema for the prey or modify"
" the way agents interact in the simulation. This is left open to "
"exploration."
msgstr ""

#: ../../source/tutorials/tutorials.rst:8
msgid "Contents:"
msgstr ""

#: ../../source/tutorials/tutorials.rst:4
msgid "Full Tutorials"
msgstr ""

#: ../../source/tutorials/tutorials.rst:6
msgid ""
"We provide tutorials that demonstrate how to train, visualize, and "
"analyze MARL policies."
msgstr ""

