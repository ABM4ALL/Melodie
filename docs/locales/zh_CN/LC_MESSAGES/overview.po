# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, open excel_source
# This file is distributed under the same license as the Melodie package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Melodie \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-12-17 11:18+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/overview.rst:4 274ed66eec2440b68df07c621c48a550
msgid "Overview"
msgstr ""

#: ../../source/overview.rst:6 c2e1d761b178420d91e87bb71c58cfdb
msgid ""
"A reinforcement learning experiment in Abmarl contains two interacting "
"components: a Simulation and a Trainer."
msgstr ""

#: ../../source/overview.rst:9 3a8fcec5925e45c78b18dda78bbbcfe8
msgid ""
"The Simulation contains agent(s) who can observe the state (or a "
"substate) of the Simulation and whose actions affect the state of the "
"simulation. The simulation is discrete in time, and at each time step "
"agents can provide actions. The simulation also produces rewards for each"
" agent that the Trainer can use to train optimal behaviors. The Agent-"
"Simulation interaction produces state-action-reward tuples (SARs), which "
"can be collected in *rollout fragments* and used to optimize agent "
"behaviors."
msgstr ""

#: ../../source/overview.rst:16 247bb4da91454e9c8eab0a97c7958464
msgid ""
"The Trainer contains policies that map agents' observations to actions. "
"Policies are one-to-many with agents, meaning that there can be multiple "
"agents using the same policy. Policies may be heuristic (i.e. coded by "
"the researcher) or trainable by the RL algorithm."
msgstr ""

#: ../../source/overview.rst:21 32e0436f161e4fdfba02f8a11bcc585c
msgid ""
"In Abmarl, the Simulation and Trainer are specified in a single Python "
"configuration file. Once these components are set up, they are passed as "
"parameters to RLlib's tune command, which will launch the RLlib "
"application and begin the training process. The training process will "
"save checkpoints to an output directory, from which the user can "
"visualize and analyze results. The following diagram demonstrates this "
"workflow."
msgstr ""

#: ../../source/overview.rst:32 012dab7a54ff4bb68dfdef5315b101d9
msgid "Abmarl usage workflow"
msgstr ""

#: ../../source/overview.rst:32 72acbc0d82054956b6d445815d9b269a
msgid ""
"Abmarl's usage workflow. An experiment configuration is used to train "
"agents' behaviors. The policies and simulation are saved to an output "
"directory. Behaviors can then be analyzed or visualized from the output "
"directory."
msgstr ""

#: ../../source/overview.rst:38 343a6b3b233f4581b231831f3bf185a0
msgid "Creating Agents and Simulations"
msgstr ""

#: ../../source/overview.rst:40 4c4bfb1582ea48589a81e3a4aeb41498
msgid ""
"Abmarl provides three interfaces for setting up an agent-based "
"simulations."
msgstr ""

#: ../../source/overview.rst:45 ab24b5890b0a404196e1ce798ba34fd5
msgid "Agent"
msgstr ""

#: ../../source/overview.rst:47 e698046b70384b51900a4d257bdcd821
msgid ""
"First, we have :ref:`Agents <api_agent>`. An agent is an object with an "
"observation and action space. Many practitioners may be accustomed to "
"gym.Env's interface, which defines the observation and action space for "
"the *simulation*. However, in heterogeneous multiagent settings, each "
"*agent* can have different spaces; thus we assign these spaces to the "
"agents and not the simulation."
msgstr ""

#: ../../source/overview.rst:53 86a9ef110162453d86961bacf51dc826
msgid "An agent can be created like so:"
msgstr ""

#: ../../source/overview.rst:65 23889fea12a94e498fbc40bd04adacf7
msgid ""
"At this level, the Agent is basically a dataclass. We have left it open "
"for our users to extend its features as they see fit."
msgstr ""

#: ../../source/overview.rst:71 711570ddcf764797b013c599d6184a5f
msgid "Agent Based Simulation"
msgstr ""

#: ../../source/overview.rst:72 ea6a5dd897684d82a7eaaf1cde21b99c
msgid ""
"Next, we define an :ref:`Agent Based Simulation <api_abs>`, or ABS for "
"short, with the ususal ``reset`` and ``step`` functions that we are used "
"to seeing in RL simulations. These functions, however, do not return "
"anything; the state information must be obtained from the getters: "
"``get_obs``, ``get_reward``, ``get_done``, ``get_all_done``, and "
"``get_info``. The getters take an agent's id as input and return the "
"respective information from the simulation's state. The ABS also contains"
" a dictionary of agents that \"live\" in the simulation."
msgstr ""

#: ../../source/overview.rst:80 b56bc5f83f9342d8881fbf55c6566a12
msgid "An Agent Based Simulation can be created and used like so:"
msgstr ""

#: ../../source/overview.rst:103 e7357f74c5904b51a4655135088cc0f4
msgid ""
"Implementations of AgentBasedSimulation should call ``finalize`` at the "
"end of its ``__init__``. Finalize ensures that all agents are configured "
"and ready to be used for training."
msgstr ""

#: ../../source/overview.rst:108 cefd7c1fc8f0433082b6653348ad6d7c
msgid ""
"Instead of treating agents as dataclasses, we could have included the "
"relevant information in the Agent Based Simulation with various "
"dictionaries. For example, we could have ``action_spaces`` and "
"``observation_spaces`` that maps agents' ids to their action spaces and "
"observation spaces, respectively. In Abmarl, we favor the dataclass "
"approach and use it throughout the package and documentation."
msgstr ""

#: ../../source/overview.rst:118 46c7fb44bd2d40f6b3634759491a6424
msgid "Simulation Managers"
msgstr ""

#: ../../source/overview.rst:120 63a6a0e73ac241caa2e90ff88234eecb
msgid ""
"The Agent Based Simulation interface does not specify an ordering for "
"agents' interactions with the simulation. This is left open to give our "
"users maximal flexibility. However, in order to interace with RLlib's "
"learning library, we provide a :ref:`Simulation Manager <api_sim>` which "
"specifies the output from ``reset`` and ``step`` as RLlib expects it. "
"Specifically,"
msgstr ""

#: ../../source/overview.rst:125 15cc346c8a1a4fc0b622febff34e833f
msgid ""
"Agents that appear in the output dictionary will provide actions at the "
"next step."
msgstr ""

#: ../../source/overview.rst:126 8a4d8ee3290e418d990c393146ecc80f
msgid ""
"Agents that are done on this step will not provide actions on the next "
"step."
msgstr ""

#: ../../source/overview.rst:128 026a1ae2899c48d7add0a1f24532e869
msgid ""
"Simulation managers are open-ended requiring only ``reset`` and ``step`` "
"with output described above. For convenience, we have provided two "
"managers: :ref:`Turn Based <api_turn_based>`, which implements turn-based"
" games; and :ref:`All Step <api_all_step>`, which has every non-done "
"agent provide actions at each step."
msgstr ""

#: ../../source/overview.rst:133 e804608a4d544679af722f685b874666
msgid "Simluation Managers \"wrap\" simulations, and they can be used like so:"
msgstr ""

#: ../../source/overview.rst:156 587e2fb71e8a4fc193c006a71adfebf5
msgid "External Integration"
msgstr ""

#: ../../source/overview.rst:158 49df93d604274026b6d72742404892d6
msgid ""
"In order to train agents in a Simulation Manager using RLlib, we must "
"wrap the simulation with either a :ref:`GymWrapper <api_gym_wrapper>` for"
" single-agent simulations (i.e. only a single entry in the `agents` dict)"
" or a :ref:`MultiAgentWrapper <api_ma_wrapper>` for multiagent "
"simulations."
msgstr ""

#: ../../source/overview.rst:166 03f47efec1a54db2af377e5dd3a2ab76
msgid "Training with an Experiment Configuration"
msgstr ""

#: ../../source/overview.rst:167 f6c86cd9efc94108b2cd2f8fc201bbcd
msgid ""
"In order to run experiments, we must define a configuration file that "
"specifies Simulation and Trainer parameters. Here is the configuration "
"file from the :ref:`Corridor tutorial<tutorial_multi_corridor>` that "
"demonstrates a simple corridor simulation with multiple agents."
msgstr ""

#: ../../source/overview.rst:231 c8bad54b5e31490bbad0789ee350f9da
msgid ""
"The simulation must be a :ref:`Simulation Manager <sim-man>` or an "
":ref:`External Wrapper <external>` as described above."
msgstr ""

#: ../../source/overview.rst:235 0fa11282c47a4fdcbe7590656ec7e10d
msgid ""
"This example has ``num_workers`` set to 7 for a computer with 8 CPU's. "
"You may need to adjust this for your computer to be `<cpu count> - 1`."
msgstr ""

#: ../../source/overview.rst:239 d8496620edc04a8381e018ae2309a705
msgid "Experiment Parameters"
msgstr ""

#: ../../source/overview.rst:240 b7bd381e1ce444959ea5dd1bd0c8c14a
msgid ""
"The strucutre of the parameters dictionary is very important. It *must* "
"have an `experiment` key which contains both the `title` of the "
"experiment and the `sim_creator` function. This function should receive a"
" config and, if appropriate, pass it to the simulation constructor. In "
"the example configuration above, we just retrun the already-configured "
"simulation. Without the title and simulation creator, Abmarl may not "
"behave as expected."
msgstr ""

#: ../../source/overview.rst:247 a5748b3e364b426f9cb7708bcc95f14f
msgid ""
"The experiment parameters also contains information that will be passed "
"directly to RLlib via the `ray_tune` parameter. See RLlib's documentation"
" for a `list of common configuration parameters "
"<https://docs.ray.io/en/releases-1.2.0/rllib-training.html#common-"
"parameters>`_."
msgstr ""

#: ../../source/overview.rst:252 e995668424eb45d1a2e96f2bc6165631
msgid "Command Line"
msgstr ""

#: ../../source/overview.rst:253 173064e6632d43fa9c64164374ca1479
msgid ""
"With the configuration file complete, we can utilize the command line "
"interface to train our agents. We simply type ``abmarl train "
"multi_corridor_example.py``, where `multi_corridor_example.py` is the "
"name of our configuration file. This will launch Abmarl, which will "
"process the file and launch RLlib according to the specified parameters. "
"This particular example should take 1-10 minutes to train, depending on "
"your compute capabilities. You can view the performance in real time in "
"tensorboard with ``tensorboard --logdir ~/abmarl_results``."
msgstr ""

#: ../../source/overview.rst:263 e6bfe78b6de44ca891910bbcb3863a21
msgid ""
"By default, the \"base\" of the output directory is the home directory, "
"and Abmarl will create the `abmarl_results` directory there. The base "
"directory can by configured in the `params` under `ray_tune` using the "
"`local_dir` parameter. This value should be a full path. For example, "
"``'local_dir': '/usr/local/scratch'``."
msgstr ""

#: ../../source/overview.rst:270 46d5267c80064215b2411897ac2fd699
msgid "Visualizing"
msgstr ""

#: ../../source/overview.rst:271 a3a52398306e403b842773514d02d8f7
msgid ""
"We can visualize the agents' learned behavior with the ``visualize`` "
"command, which takes as argument the output directory from the training "
"session stored in ``~/abmarl_results``. For example, the command"
msgstr ""

#: ../../source/overview.rst:279 eedd9289edce438a8929a40db9bf79a7
msgid ""
"will load the experiment (notice that the directory name is the "
"experiment title from the configuration file appended with a timestamp) "
"and display an animation of 5 episodes. The ``--record`` flag will save "
"the animations as `.mp4` videos in the training directory."
msgstr ""

#: ../../source/overview.rst:287 d8040bcf344a4060ae6c1d6178a1e3a9
msgid "Analyzing"
msgstr ""

#: ../../source/overview.rst:289 3c52975fe0b4490aa21789732aebf198
msgid ""
"The simulation and trainer can also be loaded into an analysis script for"
" post-processing via the ``analyze`` command. The analysis script must "
"implement the following `run` function. Below is an example that can "
"serve as a starting point."
msgstr ""

#: ../../source/overview.rst:326 8c3ebcf624ab43b0a67d196563ff806d
msgid "Analysis can then be performed using the command line interface:"
msgstr ""

#: ../../source/overview.rst:332 1bc6f54cd6b240a5b58acc24990e6a23
msgid ""
"See the :ref:`Predator Prey tutorial <tutorial_predator_prey>` for an "
"example of analyzing trained agent behavior."
msgstr ""

#: ../../source/overview.rst:336 36b747c817564a09aa74cd362f4c0f5d
msgid "Running at scale with HPC"
msgstr ""

#: ../../source/overview.rst:338 9750753a8b4149918c40bdb9f09f046e
msgid ""
"Abmarl also supports some functionality for training at scale. See the "
":ref:`magpie tutorial <tutorial_magpie>`, which provides a walkthrough "
"for launching a training experiment on multiple compute nodes with slurm."
msgstr ""

